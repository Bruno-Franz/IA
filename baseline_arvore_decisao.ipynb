{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90892ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import sys, pathlib\n",
    "    project_root = pathlib.Path('/content/drive/MyDrive/IA')\n",
    "    sys.path.append(str(project_root))\n",
    "except ModuleNotFoundError:\n",
    "    import sys, pathlib\n",
    "    sys.path.append(str(pathlib.Path().resolve()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Baseline de \u00c1rvores de Decis\u00e3o para os conjuntos Bank, Books e Flowers.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29004d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715dcc8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "except Exception:  # TensorFlow might not be installed\n",
    "    tf = None\n",
    "    tfds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02028e",
   "metadata": {},
   "source": [
    "--------------------- Bank Marketing ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1795d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "BANK_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f77f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixar_base_banco() -> Path:\n",
    "    \"\"\"Baixar o dataset banc\u00e1rio se necess\u00e1rio e retornar o caminho do CSV.\"\"\"\n",
    "    csv_path = Path(\"bank-full.csv\")\n",
    "    if csv_path.exists():\n",
    "        return csv_path\n",
    "\n",
    "    import zipfile\n",
    "    import subprocess\n",
    "\n",
    "    zip_path = Path(\"bank.zip\")\n",
    "    subprocess.run([\"wget\", \"-q\", BANK_URL, \"-O\", str(zip_path)], check=True)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extract(\"bank-full.csv\")\n",
    "    return csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessar_banco(csv_file: Path) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    # carrega o CSV e normaliza colunas\n",
    "    df = pd.read_csv(csv_file, sep=\";\")\n",
    "    binary_map = {\"yes\": 1, \"no\": 0}\n",
    "    for col in [\"default\", \"housing\", \"loan\", \"y\"]:\n",
    "        df[col] = df[col].map(binary_map)\n",
    "\n",
    "    categorical_cols = [\"job\", \"marital\", \"education\", \"contact\", \"month\", \"poutcome\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    X = df.drop(\"y\", axis=1)\n",
    "    y = df[\"y\"]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b29a8",
   "metadata": {},
   "source": [
    "--------------------- Books Reviews ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9688244",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOKS_PATH = Path(\"books_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6902ca51",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def preprocessar_livros() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    # prepara as resenhas de livros\n",
    "    df = pd.read_csv(BOOKS_PATH)\n",
    "    df = df.dropna(subset=[\"review_text\", \"label\"]).copy()\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(df[\"review_text\"].astype(str))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb5748",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "--------------------- Flowers Recognition ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessar_flores() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    # carrega o dataset de flores e extrai histogramas\n",
    "    if tfds is None:\n",
    "        raise RuntimeError(\"TensorFlow Datasets not available\")\n",
    "\n",
    "    (ds_train, ds_val, ds_test), info = tfds.load(\n",
    "        \"tf_flowers\",\n",
    "        split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "\n",
    "    # combine splits\n",
    "    ds = ds_train.concatenate(ds_val).concatenate(ds_test)\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    for img, label in tfds.as_numpy(ds):\n",
    "        # Resize to smaller size for speed\n",
    "        img_resized = tf.image.resize(img, (64, 64)).numpy().astype(\"uint8\")\n",
    "        # Compute color histogram (16 bins per channel -> 48 features)\n",
    "        hist = []\n",
    "        for i in range(3):\n",
    "            h, _ = np.histogram(img_resized[:, :, i], bins=16, range=(0, 255))\n",
    "            hist.extend(h)\n",
    "        images.append(hist)\n",
    "        labels.append(label)\n",
    "\n",
    "    X = pd.DataFrame(images)\n",
    "    y = pd.Series(labels)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50c812",
   "metadata": {},
   "source": [
    "--------------------- Evaluation ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ff7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_LIST = [\n",
    "    {\"criterion\": \"gini\", \"max_depth\": None},\n",
    "    {\"criterion\": \"entropy\", \"max_depth\": 5},\n",
    "    {\"criterion\": \"gini\", \"max_depth\": 10, \"min_samples_split\": 10},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80359e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_dataset(X, y, dataset_name: str) -> List[Dict[str, object]]:\n",
    "    # separa dados, treina \u00e1rvore e avalia\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    results = []\n",
    "    for i, params in enumerate(PARAMS_LIST, 1):\n",
    "        start = time.time()\n",
    "        clf = DecisionTreeClassifier(random_state=42, **params)\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "        duration = time.time() - start\n",
    "        report = classification_report(y_test, preds, output_dict=True, zero_division=0)\n",
    "        results.append(\n",
    "            {\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"Config\": f\"Config {i}\",\n",
    "                \"Params\": params,\n",
    "                \"Accuracy\": accuracy_score(y_test, preds),\n",
    "                \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "                \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "                \"F1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "                \"Duration\": duration,\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d630e4c",
   "metadata": {},
   "source": [
    "--------------------- Main ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55659bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dt: List[Dict[str, object]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executar_tudo() -> pd.DataFrame:\n",
    "    # executa o pipeline completo\n",
    "    global results_dt\n",
    "    results_dt = []\n",
    "\n",
    "    # Bank\n",
    "    bank_csv = baixar_base_banco()\n",
    "    X_bank, y_bank = preprocessar_banco(bank_csv)\n",
    "    results_dt.extend(avaliar_dataset(X_bank, y_bank, \"Bank\"))\n",
    "\n",
    "    # Books\n",
    "    X_books, y_books = preprocessar_livros()\n",
    "    results_dt.extend(avaliar_dataset(X_books, y_books, \"Books\"))\n",
    "\n",
    "    # Flowers\n",
    "    try:\n",
    "        X_flowers, y_flowers = preprocessar_flores()\n",
    "    except Exception as exc:\n",
    "        print(f\"Skipping flowers dataset due to error: {exc}\")\n",
    "    else:\n",
    "        results_dt.extend(avaliar_dataset(X_flowers, y_flowers, \"Flowers\"))\n",
    "\n",
    "    df = pd.DataFrame(results_dt)\n",
    "    df.to_csv(\"results.csv\", index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c297455",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = executar_tudo()\n",
    "    print(df)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
