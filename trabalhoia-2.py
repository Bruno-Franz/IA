# -*- coding: utf-8 -*-
"""TrabalhoIA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A33LupY9hRDOSC991fbZgnh-09e3NonP
"""

import pandas as pd
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip"
!wget -q $url       # baixa o ZIP
!unzip -q bank.zip  # extrai bank-full.csv e outros

df = pd.read_csv("bank-full.csv", sep=";")
df.head()

import tensorflow_datasets as tfds

(ds_train, ds_val, ds_test), info = tfds.load(
    "tf_flowers",
    split=["train[:80%]", "train[80%:90%]", "train[90%:]"],
    as_supervised=True,
    with_info=True,
)
print("Classes:", info.features["label"].names)

import zipfile
import os
import pandas as pd

# Etapa 1: Upload manual do arquivo ZIP no Colab
# from google.colab import files
# uploaded = files.upload()

# Etapa 2: Extrair o ZIP externo
outer_zip_path = '/content/bank+marketing.zip'
outer_extract_path = '/content/bank_outer'
with zipfile.ZipFile(outer_zip_path, 'r') as outer_zip:
    outer_zip.extractall(outer_extract_path)

# Etapa 3: Extrair o ZIP interno (bank.zip)
inner_zip_path = os.path.join(outer_extract_path, 'bank.zip')
inner_extract_path = '/content/bank_data'
with zipfile.ZipFile(inner_zip_path, 'r') as inner_zip:
    inner_zip.extractall(inner_extract_path)

# Etapa 4: Listar e carregar o CSV principal
print("Arquivos disponíveis:", os.listdir(inner_extract_path))

df = pd.read_csv(os.path.join(inner_extract_path, 'bank.csv'), sep=';')
df.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. Converter variáveis binárias para 0/1
df_bin = df.copy()
binary_map = {'yes': 1, 'no': 0}
for col in ['default', 'housing', 'loan', 'y']:
    df_bin[col] = df_bin[col].map(binary_map)

# 2. One-hot encoding nas variáveis categóricas
categorical_cols = ['job', 'marital', 'education', 'contact', 'month', 'poutcome']
df_encoded = pd.get_dummies(df_bin, columns=categorical_cols, drop_first=True)

# 3. Separar X (features) e y (rótulo)
X = df_encoded.drop('y', axis=1)
y = df_encoded['y']

# 4. Padronizar colunas numéricas
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5. Dividir em treino e teste (80/20)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score

# Lista de configurações de hiperparâmetros
params_list = [
    {"criterion": "gini", "max_depth": None},
    {"criterion": "entropy", "max_depth": 5},
    {"criterion": "gini", "max_depth": 10, "min_samples_split": 10}
]

# Lista para guardar resultados
results_dt = []

for i, params in enumerate(params_list, 1):
    clf = DecisionTreeClassifier(random_state=42, **params)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
    acc = accuracy_score(y_test, y_pred)

    results_dt.append({
        "Configuração": f"Configuração {i}",
        "Hiperparâmetros": params,
        "Acurácia": acc,
        "Precisão (classe 1)": report['1']['precision'],
        "Recall (classe 1)": report['1']['recall'],
        "F1-score (classe 1)": report['1']['f1-score']
    })

# Mostrar resultados
import pandas as pd
pd.DataFrame(results_dt)

!pip install -q --upgrade datasets "pyarrow<20"  # versões compatíveis

from datasets import load_dataset
b2w = load_dataset("ruanchaves/b2w-reviews01", split="train[:10%]")
print(b2w[0]["review_text"])

from huggingface_hub import list_files_info, hf_hub_download

# Entra no repositório e baixa os arquivos reais do LFS
!git -C /content/b2w-reviews01 lfs pull

import os
data_path = "/content/b2w-reviews01/data"
print("Conteúdo de", data_path, ":", os.listdir(data_path))

"""# Task
eu tenho uma base aqui que eu preciso subir para cá e que você consiga fazer a leitura desses dados para fazer um pré-processamento dos dados para depois aplicar os métodos como de árvore de decisão e quem saber uma rede neural, como eu subo?

Here is all the data you need:
"B2W-Reviews01.csv"

## Data loading

### Subtask:
Load the dataset "B2W-Reviews01.csv" into a pandas DataFrame.

**Reasoning**:
Load the dataset "B2W-Reviews01.csv" into a pandas DataFrame and display basic information about it.
"""

import pandas as pd

try:
    df = pd.read_csv('B2W-Reviews01.csv')
except UnicodeDecodeError:
    try:
        df = pd.read_csv('B2W-Reviews01.csv', encoding='latin-1')
    except UnicodeDecodeError:
        try:
            df = pd.read_csv('B2W-Reviews01.csv', encoding='utf-8')
        except Exception as e:
            print(f"Error loading the file: {e}")
            df = None

if df is not None:
    display(df.head())
    print(df.shape)
    df.info()

"""## Data exploration

### Subtask:
Explore the dataset to understand its structure, identify data types of each column, check for missing values, and examine the distribution of key variables. Determine the shape of the data and look for potential issues like inconsistencies or irrelevant columns.  Pay special attention to the 'review_comment_title' and 'review_comment_message' columns, as these might contain valuable textual information.

**Reasoning**:
Explore the dataset, identify data types, check for missing values, examine the distribution of key variables, and look for potential issues.  This involves using .shape, .info(), .describe(), examining unique values for categorical columns, calculating text lengths, and checking for duplicates.
"""

# Check the shape of the DataFrame
print("Shape of the DataFrame:", df.shape)

# Get a summary of the DataFrame
df.info()

# Descriptive statistics for numerical columns
print("\nDescriptive Statistics:")
display(df.describe())

# Examine unique values for categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    print(f"\nUnique values and frequencies for {col}:")
    print(df[col].value_counts())

# Calculate the average length of text in 'review_comment_title' and 'review_comment_message'
# Note: These columns are 'review_title' and 'review_text' in the provided data.
if 'review_title' in df.columns:
    df['review_title_length'] = df['review_title'].astype(str).apply(len)
    print(f"\nAverage length of review_title: {df['review_title_length'].mean()}")
if 'review_text' in df.columns:
    df['review_text_length'] = df['review_text'].astype(str).apply(len)
    print(f"\nAverage length of review_text: {df['review_text_length'].mean()}")

# Check for duplicated rows
duplicated_rows = df.duplicated().sum()
print(f"\nNumber of duplicated rows: {duplicated_rows}")

"""**Reasoning**:
Visualize the distribution of key variables using histograms, box plots, and count plots. Pay special attention to the `overall_rating` column. Check for potential outliers or unusual patterns in numerical and categorical features. Also, visualize the correlation matrix using a heatmap.


"""

import matplotlib.pyplot as plt
import seaborn as sns

# Overall rating distribution
plt.figure(figsize=(8, 6))
sns.countplot(x='overall_rating', data=df)
plt.title('Distribution of Overall Ratings')
plt.show()

# Histograms for numerical features
numerical_cols = df.select_dtypes(include=['number']).columns
for col in numerical_cols:
    plt.figure(figsize=(8, 6))
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.show()


# Box plots for numerical features
for col in numerical_cols:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=col, data=df)
    plt.title(f'Boxplot of {col}')
    plt.show()

# Count plots for categorical features (excluding text columns)
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    if col not in ['review_title', 'review_text']:  # Exclude text columns
        plt.figure(figsize=(12, 6)) # Adjust figure size for better readability
        sns.countplot(x=col, data=df)
        plt.title(f'Distribution of {col}')
        plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for better readability
        plt.tight_layout() # Adjust layout to prevent labels from overlapping
        plt.show()

# Correlation matrix and heatmap
plt.figure(figsize=(12, 10))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""**Reasoning**:
The previous code block failed due to a `ValueError` when trying to compute the correlation matrix because of a string column ('submission_date').  I need to handle this error by excluding non-numeric columns from the correlation calculation.  I'll also improve the plot sizes and add labels and titles to the plots.


"""

import matplotlib.pyplot as plt
import seaborn as sns

# Overall rating distribution
plt.figure(figsize=(10, 6))
sns.countplot(x='overall_rating', data=df)
plt.title('Distribution of Overall Ratings')
plt.xlabel('Overall Rating')
plt.ylabel('Count')
plt.show()

# Histograms for numerical features
numerical_cols = df.select_dtypes(include=['number']).columns
for col in numerical_cols:
    plt.figure(figsize=(10, 6))
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

# Box plots for numerical features
for col in numerical_cols:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=col, data=df)
    plt.title(f'Boxplot of {col}')
    plt.xlabel(col)
    plt.show()

# Count plots for categorical features (excluding text columns)
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    if col not in ['review_title', 'review_text']:
        plt.figure(figsize=(14, 8))
        sns.countplot(x=col, data=df)
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Count')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()

# Correlation matrix and heatmap - handle non-numeric columns
numeric_df = df.select_dtypes(include=['number'])
correlation_matrix = numeric_df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()