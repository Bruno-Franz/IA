{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c7c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import sys, pathlib\n",
    "    project_root = pathlib.Path('/content/drive/MyDrive/IA')\n",
    "    sys.path.append(str(project_root))\n",
    "except ModuleNotFoundError:\n",
    "    import sys, pathlib\n",
    "    sys.path.append(str(pathlib.Path().resolve()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script principal que executa treinamentos nos tr\u00eas conjuntos de dados.\"\"\"\n",
    "import zipfile\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643825a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "import types, nbformat\n",
    "from nbconvert import PythonExporter\n",
    "\n",
    "def _load_neural_models(nb_path='modelos_neurais.ipynb'):\n",
    "    nb = nbformat.read(nb_path, as_version=4)\n",
    "    code, _ = PythonExporter().from_notebook_node(nb)\n",
    "    module = types.ModuleType('modelos_neurais_nb')\n",
    "    exec(code, module.__dict__)\n",
    "    return module\n",
    "\n",
    "_modelos = _load_neural_models()\n",
    "treinar_mlp_keras = _modelos.treinar_mlp_keras\n",
    "treinar_cnn_texto = _modelos.treinar_cnn_texto\n",
    "treinar_cnn_lstm_texto = _modelos.treinar_cnn_lstm_texto\n",
    "treinar_gru_texto = _modelos.treinar_gru_texto\n",
    "treinar_cnn_profundo = _modelos.treinar_cnn_profundo\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to accumulate results for each training block\n",
    "results_dt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752da8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional imports for the image dataset\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    import tensorflow_datasets as tfds\n",
    "except Exception:\n",
    "    tf = None\n",
    "    tfds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620be82",
   "metadata": {},
   "source": [
    "---------------- Bank Marketing Dataset -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixar_base_banco():\n",
    "    \"\"\"Baixar e extrair o dataset de marketing banc\u00e1rio se necess\u00e1rio.\"\"\"\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip\"\n",
    "    zip_path = Path(\"bank.zip\")\n",
    "    csv_path = Path(\"bank-full.csv\")\n",
    "\n",
    "    if csv_path.exists():\n",
    "        return csv_path\n",
    "\n",
    "    try:\n",
    "        import urllib.request\n",
    "\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "            zf.extract(\"bank-full.csv\")\n",
    "    except Exception as exc:\n",
    "        print(f\"Failed to download bank dataset: {exc}\")\n",
    "        return None\n",
    "    return csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a367dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessar_banco(csv_file: Path):\n",
    "    # prepara o dataset banc\u00e1rio\n",
    "    df = pd.read_csv(csv_file, sep=\";\")\n",
    "    df = df.copy()\n",
    "\n",
    "    binary_map = {\"yes\": 1, \"no\": 0}\n",
    "    for col in [\"default\", \"housing\", \"loan\", \"y\"]:\n",
    "        df[col] = df[col].map(binary_map)\n",
    "\n",
    "    categorical_cols = [\n",
    "        \"job\",\n",
    "        \"marital\",\n",
    "        \"education\",\n",
    "        \"contact\",\n",
    "        \"month\",\n",
    "        \"poutcome\",\n",
    "    ]\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    X = df.drop(\"y\", axis=1)\n",
    "    y = df[\"y\"]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_banco(X_train, X_test, y_train, y_test):\n",
    "    # \u00e1rvore de decis\u00e3o no dataset banc\u00e1rio\n",
    "    start = time.time()\n",
    "    hyper = {\"random_state\": 42}\n",
    "    clf = DecisionTreeClassifier(**hyper)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(\"Bank Marketing dataset results:\")\n",
    "    print(classification_report(y_test, preds, zero_division=0))\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    report = classification_report(y_test, preds, output_dict=True, zero_division=0)\n",
    "    results_dt.append(\n",
    "        {\n",
    "            \"dataset\": \"Bank Marketing\",\n",
    "            \"method\": \"Decision Tree\",\n",
    "            \"hyperparameters\": hyper,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"duration\": elapsed,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_banco_mlp(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train scikit-learn MLPClassifier on the bank dataset.\"\"\"\n",
    "    # MLP do scikit-learn\n",
    "    start = time.time()\n",
    "    hyper = {\"hidden_layer_sizes\": (100,), \"max_iter\": 300, \"random_state\": 42}\n",
    "    clf = MLPClassifier(**hyper)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(\"Bank Marketing dataset results (MLP):\")\n",
    "    print(classification_report(y_test, preds, zero_division=0))\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    report = classification_report(y_test, preds, output_dict=True, zero_division=0)\n",
    "    results_dt.append(\n",
    "        {\n",
    "            \"dataset\": \"Bank Marketing\",\n",
    "            \"method\": \"MLPClassifier\",\n",
    "            \"hyperparameters\": hyper,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"duration\": elapsed,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc8518",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def treinar_banco_keras_mlp(X_train, X_test, y_train, y_test):\n",
    "    # MLP implementado em Keras\n",
    "    metrics = treinar_mlp_keras(X_train, X_test, y_train, y_test, epochs=50)\n",
    "    results_dt.append(\n",
    "        {\n",
    "            \"dataset\": \"Bank Marketing\",\n",
    "            \"method\": metrics[\"method\"],\n",
    "            \"hyperparameters\": {\"epochs\": 50},\n",
    "            \"accuracy\": metrics[\"accuracy\"],\n",
    "            \"precision\": metrics.get(\"precision\"),\n",
    "            \"recall\": metrics.get(\"recall\"),\n",
    "            \"f1\": metrics.get(\"f1\"),\n",
    "            \"duration\": metrics[\"duration\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a75d55",
   "metadata": {},
   "source": [
    "---------------- Books Reviews Dataset -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_base_livros(path: str = \"books_reviews.csv\"):\n",
    "    \"\"\"Load the Books Reviews dataset from ``path``.\n",
    "\n",
    "    The original script expected the CSV inside an ``archive`` folder,\n",
    "    but the repository already ships ``books_reviews.csv`` at the project\n",
    "    root. The default path was adjusted so ``executar_tudo()`` works out of the\n",
    "    box without additional configuration.\n",
    "    \"\"\"\n",
    "    # carrega o CSV de resenhas\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480fa614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessar_livros(df: pd.DataFrame):\n",
    "    # gera representa\u00e7\u00f5es TF-IDF\n",
    "    df = df.dropna(subset=[\"review_text\", \"label\"]).copy()\n",
    "    X = df[\"review_text\"].astype(str)\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "    return X_train_vec, X_test_vec, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessar_livros_seq(df: pd.DataFrame, num_words: int = 10000,\n",
    "                            max_len: int = 200):\n",
    "    \"\"\"Tokenize text reviews for neural models.\"\"\"\n",
    "    # produz sequ\u00eancias de tokens\n",
    "    df = df.dropna(subset=[\"review_text\", \"label\"]).copy()\n",
    "    X = df[\"review_text\"].astype(str)\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(num_words=num_words,\n",
    "                                                   oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    train_seq = keras.preprocessing.sequence.pad_sequences(\n",
    "        train_seq, maxlen=max_len, padding=\"post\", truncating=\"post\"\n",
    "    )\n",
    "    test_seq = keras.preprocessing.sequence.pad_sequences(\n",
    "        test_seq, maxlen=max_len, padding=\"post\", truncating=\"post\"\n",
    "    )\n",
    "\n",
    "    vocab_size = min(num_words, len(tokenizer.word_index) + 1)\n",
    "    return train_seq, test_seq, y_train, y_test, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22920026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_livros(X_train, X_test, y_train, y_test):\n",
    "    # regress\u00e3o log\u00edstica nas resenhas\n",
    "    start = time.time()\n",
    "    hyper = {\"max_iter\": 1000}\n",
    "    clf = LogisticRegression(**hyper)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(\"Books Reviews dataset results:\")\n",
    "    print(classification_report(y_test, preds, zero_division=0))\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    report = classification_report(y_test, preds, output_dict=True, zero_division=0)\n",
    "    results_dt.append(\n",
    "        {\n",
    "            \"dataset\": \"Books Reviews\",\n",
    "            \"method\": \"Logistic Regression\",\n",
    "            \"hyperparameters\": hyper,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"duration\": elapsed,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30155b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_livros_cnn(train_seq, test_seq, y_train, y_test, vocab_size):\n",
    "    # CNN simples para texto\n",
    "    metrics = treinar_cnn_texto(train_seq, test_seq, y_train, y_test, vocab_size)\n",
    "    results_dt.append(\n",
    "        {\n",
    "            \"dataset\": \"Books Reviews\",\n",
    "            \"method\": metrics[\"method\"],\n",
    "            \"hyperparameters\": {\"vocab_size\": vocab_size},\n",
    "            \"accuracy\": metrics[\"accuracy\"],\n",
    "            \"precision\": metrics.get(\"precision\"),\n",
    "            \"recall\": metrics.get(\"recall\"),\n",
    "            \"f1\": metrics.get(\"f1\"),\n",
    "            \"duration\": metrics[\"duration\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72ffce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_livros_cnn_lstm(train_seq, test_seq, y_train, y_test, vocab_size):\n",
    "    # CNN combinada com LSTM\n",
    "    metrics = treinar_cnn_lstm_texto(train_seq, test_seq, y_train, y_test, vocab_size)\n",
    "    results_dt.append(\n",
    "        {\n",
    "            \"dataset\": \"Books Reviews\",\n",
    "            \"method\": metrics[\"method\"],\n",
    "            \"hyperparameters\": {\"vocab_size\": vocab_size},\n",
    "            \"accuracy\": metrics[\"accuracy\"],\n",
    "            \"precision\": metrics.get(\"precision\"),\n",
    "            \"recall\": metrics.get(\"recall\"),\n",
    "            \"f1\": metrics.get(\"f1\"),\n",
    "            \"duration\": metrics[\"duration\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330934c5",
   "metadata": {},
   "source": [
    "---------------- TF Flowers Dataset -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_tf_flores():\n",
    "    # carrega dataset de flores do TensorFlow\n",
    "    if tfds is None:\n",
    "        print(\"TensorFlow datasets is not available.\")\n",
    "        return None\n",
    "    try:\n",
    "        return tfds.load(\n",
    "            \"tf_flowers\",\n",
    "            split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "            as_supervised=True,\n",
    "            with_info=True,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(f\"Failed to load tf_flowers: {exc}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83414313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessar_tf_flores(ds_train, ds_val, ds_test):\n",
    "    # normaliza e redimensiona imagens\n",
    "    IMG_SIZE = (180, 180)\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    def resize_norm(image, label):\n",
    "        image = tf.image.resize(image, IMG_SIZE)\n",
    "        image = image / 255.0\n",
    "        return image, label\n",
    "\n",
    "    ds_train = (\n",
    "        ds_train.map(resize_norm).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    ds_val = ds_val.map(resize_norm).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    ds_test = ds_test.map(resize_norm).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds_train, ds_val, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_flores(ds_train, ds_val, ds_test, num_classes):\n",
    "    # CNN simples para o dataset de flores\n",
    "    start = time.time()\n",
    "    hyper = {\"epochs\": 3}\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(32, 3, activation=\"relu\", input_shape=(180, 180, 3)),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, 3, activation=\"relu\"),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation=\"relu\"),\n",
    "            layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    model.fit(ds_train, validation_data=ds_val, epochs=hyper[\"epochs\"])\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch, labels in ds_test:\n",
    "        preds = model.predict(batch)\n",
    "        y_pred.extend(preds.argmax(axis=1))\n",
    "        y_true.extend(labels.numpy())\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(\"TF Flowers dataset results:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    results_dt.append(\n",
    "        {\n",
    "            \"dataset\": \"TF Flowers\",\n",
    "            \"method\": \"Simple CNN\",\n",
    "            \"hyperparameters\": hyper,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"duration\": elapsed,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_flores_profundo(ds_train, ds_val, ds_test, num_classes):\n",
    "    # vers\u00e3o mais profunda da CNN\n",
    "    metrics = treinar_cnn_profundo(ds_train, ds_val, ds_test, num_classes)\n",
    "    results_dt.append(\n",
    "        {\n",
    "            \"dataset\": \"TF Flowers\",\n",
    "            \"method\": metrics[\"method\"],\n",
    "            \"hyperparameters\": {\"epochs\": 20},\n",
    "            \"accuracy\": metrics[\"accuracy\"],\n",
    "            \"precision\": metrics.get(\"precision\"),\n",
    "            \"recall\": metrics.get(\"recall\"),\n",
    "            \"f1\": metrics.get(\"f1\"),\n",
    "            \"duration\": metrics[\"duration\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe96c30",
   "metadata": {},
   "source": [
    "---------------- Main Script -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba92e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executar_tudo():\n",
    "    # orquestra todos os experimentos\n",
    "    global results_dt\n",
    "    results_dt = []\n",
    "    # Bank Marketing\n",
    "    csv_file = baixar_base_banco()\n",
    "    if csv_file is not None:\n",
    "        X_train, X_test, y_train, y_test = preprocessar_banco(csv_file)\n",
    "        treinar_banco(X_train, X_test, y_train, y_test)\n",
    "        treinar_banco_mlp(X_train, X_test, y_train, y_test)\n",
    "        treinar_banco_keras_mlp(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Books Reviews\n",
    "    books_df = carregar_base_livros()\n",
    "    X_train, X_test, y_train, y_test = preprocessar_livros(books_df)\n",
    "    treinar_livros(X_train, X_test, y_train, y_test)\n",
    "    train_seq, test_seq, y_train_seq, y_test_seq, vocab = preprocessar_livros_seq(books_df)\n",
    "    treinar_livros_cnn(train_seq, test_seq, y_train_seq, y_test_seq, vocab)\n",
    "    treinar_livros_cnn_lstm(train_seq, test_seq, y_train_seq, y_test_seq, vocab)\n",
    "\n",
    "    # TF Flowers\n",
    "    flowers = carregar_tf_flores()\n",
    "    if flowers is not None and tf is not None:\n",
    "        (ds_train, ds_val, ds_test), info = flowers\n",
    "        ds_train, ds_val, ds_test = preprocessar_tf_flores(ds_train, ds_val, ds_test)\n",
    "        treinar_flores(ds_train, ds_val, ds_test, info.features[\"label\"].num_classes)\n",
    "        treinar_flores_profundo(ds_train, ds_val, ds_test, info.features[\"label\"].num_classes)\n",
    "\n",
    "    # Consolidate results and export to CSV\n",
    "    df = pd.DataFrame(results_dt)\n",
    "    df.to_csv(\"results.csv\", index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a774573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_dataset(df: pd.DataFrame, target: str) -> List[Dict[str, float]]:\n",
    "    \"\"\"Train simple models on *df* and return metrics.\"\"\"\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"GaussianNB\": GaussianNB(),\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_test, preds, average=\"weighted\", zero_division=0\n",
    "        )\n",
    "        results.append(\n",
    "            {\n",
    "                \"method\": name,\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ec684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agregar_resultados(csv_path: str) -> Mapping[str, pd.DataFrame]:\n",
    "    \"\"\"Group ``results.csv`` by dataset and method and average metrics.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"duration\"]\n",
    "    grouped = df.groupby([\"dataset\", \"method\"])[metrics].mean().reset_index()\n",
    "\n",
    "    result: Dict[str, pd.DataFrame] = {}\n",
    "    for dataset, table in grouped.groupby(\"dataset\"):\n",
    "        result[dataset] = table.drop(columns=\"dataset\").reset_index(drop=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_tabelas(csv_path: str, out_dir: str = \".\", prefix: str = \"table\") -> None:\n",
    "    \"\"\"Create per-dataset tables in CSV and Markdown format.\"\"\"\n",
    "    tables = agregar_resultados(csv_path)\n",
    "    out = Path(out_dir)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for dataset, table in tables.items():\n",
    "        slug = dataset.lower().replace(\" \", \"_\")\n",
    "        table.to_csv(out / f\"{prefix}_{slug}.csv\", index=False)\n",
    "        try:\n",
    "            markdown = table.to_markdown(index=False)\n",
    "        except ImportError:\n",
    "            markdown = table.to_csv(index=False)\n",
    "        with open(out / f\"{prefix}_{slug}.md\", \"w\", encoding=\"utf-8\") as fh:\n",
    "            fh.write(markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    executar_tudo()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
